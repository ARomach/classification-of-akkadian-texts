{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../ORACC-catalogues-030524.csv\",\n",
    "                  encoding=\"utf-8\", index_col=\"_ - index\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(corpus, tfidf, analyzer, ngram_range, max_df, min_df, max_features, file_keys):\n",
    "\n",
    "    if tfidf:\n",
    "        vectorizer = TfidfVectorizer(input=\"content\", lowercase=True, analyzer=analyzer, token_pattern=r\"(?u)\\b\\w+\\b\", ngram_range=ngram_range, max_df=max_df, min_df=min_df, max_features=max_features)\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(input=\"content\", lowercase=True, analyzer=analyzer, token_pattern=r\"(?u)\\b\\w+\\b\", ngram_range=ngram_range, max_df=max_df, min_df=min_df, max_features=max_features)\n",
    "    \n",
    "    counts = vectorizer.fit_transform(corpus).toarray()\n",
    "    # saving the vocab used for vectorization, and switching the dictionary so that the feature index is the key\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    switched_vocab = {value: key for key, value in vocab.items()}\n",
    "    # adding the vocab words to the counts dataframe for easier viewing.\n",
    "    column_names = []\n",
    "    x = 0\n",
    "    while x < len(switched_vocab):\n",
    "        column_names.append(switched_vocab[x])\n",
    "        x += 1\n",
    "\n",
    "    counts_df = pd.DataFrame(counts, index=file_keys, columns=column_names)\n",
    "\n",
    "    return (counts, counts_df)\n",
    "\n",
    "\n",
    "def prepare_data(df, category, word_level, tfidf=False, max_df=.85, min_df=20, max_features=None):\n",
    "    \n",
    "    df = df[[category, word_level]].copy()\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"y\"] = label_encoder.fit_transform(df[category])\n",
    "    \n",
    "    clean_texts = []\n",
    "    for text in df[word_level].tolist():\n",
    "        clean_texts.append(text.replace(\"UNK\", \"\").replace(\"X\", \"\").replace(\"-\", \"\"))\n",
    "        \n",
    "    if word_level != \"unseg_uni\":\n",
    "        counts, counts_df = vectorize(clean_texts, tfidf, \"word\", (1,1), max_df, min_df, max_features, df.index)\n",
    "    else:\n",
    "        counts, counts_df = vectorize(clean_texts, tfidf, \"char\", (1,3), max_df, min_df, max_features, df.index)\n",
    "    \n",
    "    X = counts_df\n",
    "    y = df[\"y\"]\n",
    "    \n",
    "    return X, y, label_encoder.classes_\n",
    "\n",
    "def train_multinomialNB(X, y):\n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    weighted_f1_scores = [] \n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=0)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        weighted_f1_scores.append(weighted_f1)\n",
    "    \n",
    "    precision_scores = np.array(precision_scores).mean(axis=0)\n",
    "    recall_scores = np.array(recall_scores).mean(axis=0)\n",
    "    f1_scores = np.array(f1_scores).mean(axis=0)\n",
    "    weighted_f1_scores = np.array(weighted_f1_scores).mean()\n",
    "    \n",
    "    return precision_scores, recall_scores, f1_scores, weighted_f1_scores\n",
    "\n",
    "def save_results_table(results, classes, category, word_level, save_name):\n",
    "    \n",
    "    weighted_f1 = results[3]\n",
    "    nan_array = np.full(len(results[0])-1, np.nan)\n",
    "    weighted_f1 = np.append(np.array(weighted_f1), nan_array)\n",
    "    new_results = [results[0], results[1], results[2], weighted_f1]\n",
    "    \n",
    "    df = pd.DataFrame(new_results, index=[\"Precision\", \"Recall\", \"F1\", \"Weighted F1\"], columns=classes).transpose()\n",
    "    df.to_csv(f\"../reports/MultinomialNB/{save_name}_{category}_{word_level}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_save_results(save_name):\n",
    "    categories = [\"supergenre_160424\", \"superperiod_160424\", \"superprovenience_160424\"]\n",
    "    word_levels = [\"lemm\", \"norm\", \"seg_uni\", \"unseg_uni\"]\n",
    "\n",
    "    results = []\n",
    "    for cat in categories:\n",
    "        # filter out small examples in specific categories:\n",
    "        if cat == 'superperiod_160424':\n",
    "            filtered_df = df[(df[\"superperiod_160424\"]!=\"Unknown\")&(df[\"superperiod_160424\"]!=\"First Millennium\")].copy()\n",
    "        elif cat == 'superprovenience_160424':\n",
    "            filtered_df = df[(df[\"superprovenience_160424\"]!=\"East\")&(df[\"superprovenience_160424\"]!=\"Unknown\")].copy()\n",
    "        else:\n",
    "            filtered_df = df.copy()\n",
    "        \n",
    "        for level in word_levels:\n",
    "            print(\"------------------------------\")\n",
    "            print(f\"current job: {cat, level}\")\n",
    "            X, y, classes = prepare_data(filtered_df, cat, level, tfidf=True)\n",
    "            print(\"started scoring\")\n",
    "            scores = train_multinomialNB(X, y)\n",
    "            save_results_table(scores, classes, cat, level, save_name)\n",
    "            results.append((cat, level, scores[-1]))\n",
    "    results_df = pd.DataFrame(results, columns=[\"Category\", \"Word Level\", \"Weighted F1\"])\n",
    "    results_df.to_csv(f\"../reports/MultinomialNB/{save_name}_WeightedF1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the only parameter this function needs is a unique name to save the results in the reports folder with.\n",
    "run_and_save_results(\"unique_id_for_saving_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (akkadian-classification)",
   "language": "python",
   "name": "akkadian-classification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
