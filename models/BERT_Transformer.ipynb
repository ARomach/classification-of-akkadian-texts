{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and uploads"
      ],
      "metadata": {
        "id": "nTohFflbNL9A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "742RRAhZM0rj"
      },
      "outputs": [],
      "source": [
        "# for Google Colab\n",
        "# install specific versions for compatibility\n",
        "!pip install accelerate==0.23.0 -U\n",
        "!pip install transformers==4.33.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install for Japanese BERT\n",
        "!pip install \"fugashi[unidic-lite]\""
      ],
      "metadata": {
        "id": "sQP2se85xv9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from transformers import BertTokenizer, Trainer, TrainingArguments, BertForSequenceClassification, AutoConfig, TrainerCallback\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from torch.utils.data import Dataset\n",
        "import accelerate\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "NwI5ANgxNU91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import for Japanese BERT\n",
        "import fugashi"
      ],
      "metadata": {
        "id": "IoTjg5pIx1uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create directories to save data\n",
        "directories = [\"tokenizers/\", \"results/\", \"logs/\"]\n",
        "\n",
        "for dir_name in directories:\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.makedirs(dir_name)\n",
        "\n",
        "# upload dataset from github\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ARomach/classification-of-akkadian-texts/main/ORACC-catalogues-030524.csv\",\n",
        "                  encoding=\"utf-8\", index_col=\"_ - index\")"
      ],
      "metadata": {
        "id": "DNFvTI0vOu9j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_tokenizers(tokenizer_name):\n",
        "\n",
        "    original_tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "    for word_level in (\"lemm\", \"norm\", \"seg_uni\", \"unseg_uni\"):\n",
        "        # Load your trained tokenizer\n",
        "        new_tokenizer = Tokenizer.from_file(f\"tokenizers/{word_level}_wordpiece.tokenizer\")\n",
        "        new_vocab = [token for token in new_tokenizer.get_vocab().keys() if token not in\n",
        "                     original_tokenizer.get_vocab().keys()]\n",
        "\n",
        "        # Add new tokens to the original tokenizer and save it\n",
        "        num_added_toks = original_tokenizer.add_tokens(new_vocab)\n",
        "        print(\"Added\", num_added_toks, \"new tokens for\", word_level, \"with\", tokenizer_name)\n",
        "\n",
        "        # Save the updated tokenizer\n",
        "        original_tokenizer.save_pretrained(f\"tokenizers/updated_{tokenizer_name}_{word_level}\")"
      ],
      "metadata": {
        "id": "B5b5MRCvUEoG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune BERT Transformer"
      ],
      "metadata": {
        "id": "84wiKV7jPAmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset\n",
        "class AkkadianDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "class MetricsCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.train_losses = []\n",
        "        self.eval_losses = []\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        # Log training loss\n",
        "        if state.log_history and \"loss\" in state.log_history[-1]:\n",
        "            self.train_losses.append(state.log_history[-1][\"loss\"])\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        # Log evaluation loss after each evaluation\n",
        "        if metrics and \"eval_loss\" in metrics:\n",
        "            self.eval_losses.append(metrics[\"eval_loss\"])"
      ],
      "metadata": {
        "id": "bEjJVNMWO2b2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_model(model_name, tokenizer_name):\n",
        "\n",
        "    # Loop through prediction categories\n",
        "    for category in (\"supergenre_160424\", \"superperiod_160424\", \"superprovenience_160424\"):\n",
        "        # filter out small examples in specific categories:\n",
        "        if category == \"superperiod_160424\":\n",
        "            filtered_df = df[(df[\"superperiod_160424\"]!=\"Unknown\")&(df[\"superperiod_160424\"]!=\"First Millennium\")].copy()\n",
        "        elif category == \"superprovenience_160424\":\n",
        "            filtered_df = df[(df[\"superprovenience_160424\"]!=\"East\")&(df[\"superprovenience_160424\"]!=\"Unknown\")].copy()\n",
        "        else:\n",
        "            filtered_df = df.copy()\n",
        "\n",
        "        filtered_df[\"labels\"] = filtered_df[category].astype(\"category\").cat.codes\n",
        "\n",
        "        sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
        "        for train_index, test_index in sss1.split(filtered_df, filtered_df[\"labels\"]):\n",
        "            train_val_df, test_df = filtered_df.iloc[train_index], filtered_df.iloc[test_index]\n",
        "\n",
        "        sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.22, random_state=42)\n",
        "        for train_index, test_index in sss2.split(train_val_df, train_val_df[\"labels\"]):\n",
        "            train_df, val_df = train_val_df.iloc[train_index], train_val_df.iloc[test_index]\n",
        "\n",
        "        # Loop through word levels\n",
        "        for word_level in (\"lemm\", \"norm\", \"seg_uni\", \"unseg_uni\"):\n",
        "            # Encode the texts and prepare labels\n",
        "            tokenizer = BertTokenizer.from_pretrained(f\"tokenizers/updated_{tokenizer_name}_{word_level}\")\n",
        "\n",
        "            print(\"tokenizing...\")\n",
        "            train_encodings = tokenizer(train_df[word_level].tolist(), truncation=True, padding=True, max_length=128)\n",
        "            val_encodings = tokenizer(val_df[word_level].tolist(), truncation=True, padding=True, max_length=128)\n",
        "            test_encodings = tokenizer(test_df[word_level].tolist(), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "            # Get labels\n",
        "            train_labels = train_df[\"labels\"].tolist()\n",
        "            val_labels = val_df[\"labels\"].tolist()\n",
        "            test_labels = test_df[\"labels\"].tolist()\n",
        "\n",
        "            # Create Datasets\n",
        "            print(\"creating datasets...\")\n",
        "            train_dataset = AkkadianDataset(train_encodings, train_labels)\n",
        "            val_dataset = AkkadianDataset(val_encodings, val_labels)\n",
        "            test_dataset = AkkadianDataset(test_encodings, test_labels)\n",
        "\n",
        "            # Load model\n",
        "            model = BertForSequenceClassification.from_pretrained(model_name,\n",
        "                                                                  num_labels=filtered_df[\"labels\"].nunique())\n",
        "            model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)\n",
        "\n",
        "            # Setting results and logs subdirectories\n",
        "            model_name_to_save = model_name.replace(\"/\", \"-\")\n",
        "            if not os.path.exists(f\"results/{model_name_to_save}_{category}_{word_level}/\"):\n",
        "                os.mkdir(f\"results/{model_name_to_save}_{category}_{word_level}/\")\n",
        "            if not os.path.exists(f\"logs/{model_name_to_save}_{category}_{word_level}/\"):\n",
        "                os.mkdir(f\"logs/{model_name_to_save}_{category}_{word_level}/\")\n",
        "\n",
        "            # Set up training arguments\n",
        "            training_args = TrainingArguments(\n",
        "                save_strategy=\"no\",            # change to yes or remove to save models\n",
        "                output_dir=f\"results/{model_name_to_save}_{category}_{word_level}/\",\n",
        "                num_train_epochs=10,\n",
        "                per_device_train_batch_size=8,\n",
        "                per_device_eval_batch_size=8,\n",
        "                warmup_steps=500,\n",
        "                weight_decay=0.01,\n",
        "                logging_dir=f\"logs/{model_name_to_save}_{category}_{word_level}/\",\n",
        "                logging_steps=10,\n",
        "                evaluation_strategy=\"epoch\",\n",
        "                load_best_model_at_end=False  # change to True to save the final model\n",
        "            )\n",
        "\n",
        "            # Initialize callback\n",
        "            metrics_callback = MetricsCallback()\n",
        "\n",
        "            # Initialize the Trainer\n",
        "            trainer = Trainer(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=train_dataset,\n",
        "                eval_dataset=val_dataset,\n",
        "                callbacks=[metrics_callback]\n",
        "            )\n",
        "\n",
        "            # Train the model\n",
        "            print(\"training...\")\n",
        "            trainer.train()\n",
        "\n",
        "            # Make predictions\n",
        "            predictions, labels, _ = trainer.predict(test_dataset)\n",
        "\n",
        "            # Decode predictions\n",
        "            predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "            accuracy = accuracy_score(labels, predictions)\n",
        "            target_names = filtered_df[category].astype(\"category\").cat.categories.tolist()\n",
        "            report = classification_report(labels, predictions, target_names=target_names)\n",
        "\n",
        "            # Save to file and print\n",
        "            with open(f\"Report_{model_name_to_save}_{category}_{word_level}.txt\", \"w\") as file:\n",
        "                file.write(f\"Accuracy: {str(accuracy)}\\n\")\n",
        "                file.write(f\"Classification Report:\\n\")\n",
        "                file.write(report+\"\\n\")\n",
        "\n",
        "                file.write(\"Training Loss:\\n\")\n",
        "                file.write(f\"{metrics_callback.train_losses}\\n\")\n",
        "                file.write(\"Validation Loss:\\n\")\n",
        "                file.write(f\"{metrics_callback.eval_losses}\\n\")\n",
        "\n",
        "                file.write(\"Train, Validation, Test split:\\n\")\n",
        "                file.write(f\"Train: {train_df.shape[0]}\\n\")\n",
        "                file.write(f\"Validation: {val_df.shape[0]}\\n\")\n",
        "                file.write(f\"Test: {test_df.shape[0]}\\n\")\n",
        "\n",
        "            print(f\"{model_name}_{category}_{word_level} Accuracy:\", accuracy)\n",
        "            print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "id": "zSfxtbKWPEmB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "W6dcA-kQANOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare tokenizers\n",
        "\n",
        "The code block below prepares tokens that will be saves in the tokenizers folder. You only need to run this once."
      ],
      "metadata": {
        "id": "VbyukFkdO6dJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a tokenizer with WordPiece\n",
        "tokenizer = Tokenizer(WordPiece(unk_token=\"UNK\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Create a trainer\n",
        "trainer = WordPieceTrainer(\n",
        "    vocab_size=20000,\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\"UNK\", \"PN\", \"RN\", \"DN\", \"GN\", \"MN\", \"SN\", \"NUM\", \"X\"]\n",
        ")\n",
        "\n",
        "for word_level in (\"lemm\", \"norm\", \"seg_uni\", \"unseg_uni\"):\n",
        "    # Train the tokenizer\n",
        "    texts = df[word_level].tolist()\n",
        "    tokenizer.train_from_iterator(texts, trainer)\n",
        "\n",
        "    # Save the tokenizer\n",
        "    tokenizer.save(f\"tokenizers/{word_level}_wordpiece.tokenizer\")"
      ],
      "metadata": {
        "id": "kKqjaqviO5tn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine tokenizers\n",
        "\n",
        "The code block below uses the combine_tokenizers function to combine between the Akkadian tokenizers created above on each of the word level, and the original tokenizer of the model that is being used for pre-training. The resulting tokenizer is saved in the tokenizers folder.\n",
        "\n",
        "Models that were used (from Hugging Face):\n",
        "\n",
        "- `bert-base-multilingual-cased`\n",
        "- `CAMeL-Lab/bert-base-arabic-camelbert-mix`\n",
        "- `tohoku-nlp/bert-base-japanese-char-v2`"
      ],
      "metadata": {
        "id": "5mDKWw2wAU9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combine_tokenizers(\"bert-base-multilingual-cased\")"
      ],
      "metadata": {
        "id": "kEwxamhlUTu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model\n",
        "\n",
        "The code block below fine-tunes an existing BERT model, on all classification categories and all word level at once. Runtime with GPU is around 3.5-4 hours.\n",
        "\n",
        "Function takes two parameters, `model_name` and `tokenizer_name`. Give the name of the original model from HuggingFace. If the tokenizers have not been combined yet, it will throw and error (see code block above)."
      ],
      "metadata": {
        "id": "9Nr57dx7AXV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_model(model_name=\"bert-base-multilingual-cased\",\n",
        "                tokenizer_name=\"bert-base-multilingual-cased\")"
      ],
      "metadata": {
        "id": "JrssNeubtG4b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}